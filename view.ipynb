{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2ccff5-27e2-4306-b14b-ccde7603ce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial time split sizes -> train: (7576, 6)  test: (1894, 6)\n",
      "Initial class counts (train/test): {2.0: 7243, 1.0: 270, 0.0: 63} {2.0: 1894}\n",
      "WARNING: time-based test has <2 classes. Falling back to stratified random split.\n",
      "Stratified split sizes -> train: (7576, 6)  test: (1894, 6)\n",
      "Class counts (train/test): {2.0: 7310, 1.0: 216, 0.0: 50} {2.0: 1827, 1.0: 54, 0.0: 13}\n",
      "Any NaN left in X_tr after impute? False\n",
      "Any NaN left in X_te after impute? False\n",
      "Final feature dtypes:\n",
      " CO(GT)         float64\n",
      "NO2(GT)        float64\n",
      "PT08.S5(O3)    float64\n",
      "T              float64\n",
      "RH             float64\n",
      "AH             float64\n",
      "dtype: object\n",
      "Final class distribution (train):\n",
      " AQI_next\n",
      "2    7310\n",
      "1     216\n",
      "0      50\n",
      "Name: count, dtype: int64\n",
      "Final class distribution (test):\n",
      " AQI_next\n",
      "2    1827\n",
      "1      54\n",
      "0      13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "RF accuracy: 0.953\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.08      0.08        13\n",
      "           1       0.11      0.04      0.06        54\n",
      "           2       0.97      0.99      0.98      1827\n",
      "\n",
      "    accuracy                           0.95      1894\n",
      "   macro avg       0.39      0.37      0.37      1894\n",
      "weighted avg       0.94      0.95      0.94      1894\n",
      "\n",
      "RF confusion matrix:\n",
      " [[   1    0   12]\n",
      " [   1    2   51]\n",
      " [   9   16 1802]]\n",
      "\n",
      "Training stacking ensemble...\n",
      "Done training.\n",
      "\n",
      "Stacking ensemble accuracy: 0.9641\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.00      0.00      0.00        54\n",
      "           2       0.97      1.00      0.98      1827\n",
      "\n",
      "    accuracy                           0.96      1894\n",
      "   macro avg       0.32      0.33      0.33      1894\n",
      "weighted avg       0.93      0.96      0.95      1894\n",
      "\n",
      "Ensemble confusion matrix:\n",
      " [[   0    0   13]\n",
      " [   1    0   53]\n",
      " [   1    0 1826]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\91955\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 247, in _count_physical_cores\n",
      "    cpu_count_physical = _count_physical_cores_win32()\n",
      "  File \"C:\\Users\\91955\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 299, in _count_physical_cores_win32\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\91955\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91955\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\91955\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved ensemble + imputer to: C:/Users/91955/Desktop/infosys_aqi_project/models/stack_ensemble_aqi.joblib\n",
      "\n",
      "Sample next-hour prediction:\n",
      "   CO(GT)  NO2(GT)  PT08.S5(O3)     T    RH    AH\n",
      "0     2.8     55.0          650  11.2  36.0  0.75\n",
      "Predicted next-hour class: 2 Unhealthy\n",
      "Class probs: [0.00511942 0.02466155 0.97021903]\n"
     ]
    }
   ],
   "source": [
    "# ===== Robust fixed pipeline (paste this entire cell) =====\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Paths\n",
    "csv_path = 'C:/Users/91955/Desktop/infosys_aqi_project/data/AirQualityUCI.csv'\n",
    "model_path = r\"C:/Users/91955/Desktop/infosys_aqi_project/models/stack_ensemble_aqi.joblib\"\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "# --- 1) Load and canonicalize raw data ---\n",
    "df = pd.read_csv(csv_path, sep=';')\n",
    "df = df.replace(-200, np.nan)\n",
    "\n",
    "# Remove trailing/leading spaces and stray semicolons in object columns\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == object:\n",
    "        df[c] = df[c].astype(str).str.strip().str.replace(';','', regex=False)\n",
    "\n",
    "# --- 2) Convert numeric columns you plan to use to numeric (coerce invalid -> NaN) ---\n",
    "candidate_features = ['CO(GT)', 'NO2(GT)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n",
    "for c in candidate_features:\n",
    "    if c in df.columns:\n",
    "        # handle categorical dtype as well\n",
    "        if pd.api.types.is_categorical_dtype(df[c]):\n",
    "            df[c] = df[c].cat.as_ordered().codes\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# --- 3) Compute IAQI buckets (your deterministic rules) ---\n",
    "# safe guards: if column missing, create NaN/placeholder\n",
    "df['IAQI_CO'] = np.nan\n",
    "df['IAQI_NO2'] = np.nan\n",
    "df['IAQI_O3'] = np.nan\n",
    "\n",
    "if 'CO(GT)' in df.columns:\n",
    "    df.loc[:, 'IAQI_CO'] = np.where(df['CO(GT)'] <= 2, 25,\n",
    "                                    np.where(df['CO(GT)'] <= 4, 75, 125))\n",
    "\n",
    "if 'NO2(GT)' in df.columns:\n",
    "    df.loc[:, 'IAQI_NO2'] = np.where(df['NO2(GT)'] <= 40, 25,\n",
    "                                     np.where(df['NO2(GT)'] <= 80, 75, 125))\n",
    "\n",
    "if 'PT08.S5(O3)' in df.columns:\n",
    "    try:\n",
    "        # qcut on non-NA values only; use duplicates='drop' to prevent errors\n",
    "        df.loc[:, 'O3_bucket'] = pd.qcut(df['PT08.S5(O3)'], q=3, labels=[0,1,2], duplicates='drop')\n",
    "        df.loc[:, 'IAQI_O3'] = df['O3_bucket'].map({0:25, 1:75, 2:125})\n",
    "    except Exception:\n",
    "        df.loc[:, 'IAQI_O3'] = 75\n",
    "else:\n",
    "    df.loc[:, 'IAQI_O3'] = 0\n",
    "\n",
    "# --- 4) AQI and class mapping ---\n",
    "df.loc[:, 'AQI_calc'] = df[['IAQI_CO','IAQI_NO2','IAQI_O3']].max(axis=1)\n",
    "\n",
    "def aqi_to_label(v):\n",
    "    if pd.isna(v): return np.nan\n",
    "    if v <= 50: return 0\n",
    "    if v <= 100: return 1\n",
    "    return 2\n",
    "\n",
    "df.loc[:, 'AQI_class'] = df['AQI_calc'].apply(aqi_to_label)\n",
    "\n",
    "# --- 5) Create next-hour target so target is not identical to features (prevents trivial mapping) ---\n",
    "df = df.reset_index(drop=True)\n",
    "df.loc[:, 'AQI_next'] = df['AQI_class'].shift(-1)\n",
    "df_time = df.dropna(subset=['AQI_next']).copy()\n",
    "df_time.loc[:, 'AQI_next'] = df_time['AQI_next'].astype(int)\n",
    "\n",
    "# --- 6) Build feature matrix & target ---\n",
    "features = [f for f in candidate_features if f in df_time.columns]\n",
    "if len(features) == 0:\n",
    "    raise RuntimeError(\"No feature columns found. Check CSV columns.\")\n",
    "\n",
    "X_time = df_time[features].copy()\n",
    "y_time = df_time['AQI_next'].copy()\n",
    "\n",
    "# --- 7) Ensure feature columns are numeric (force) BEFORE split to avoid strange categorical dtypes ---\n",
    "for c in X_time.columns:\n",
    "    # strip & coerce once more\n",
    "    X_time.loc[:, c] = pd.to_numeric(X_time[c], errors='coerce')\n",
    "\n",
    "# --- 8) Time-based split (chronological). If later test contains <2 classes, fallback to stratified split. ---\n",
    "split_idx = int(0.8 * len(X_time))\n",
    "X_tr, X_te = X_time.iloc[:split_idx].copy(), X_time.iloc[split_idx:].copy()\n",
    "y_tr, y_te = y_time.iloc[:split_idx].copy(), y_time.iloc[split_idx:].copy()\n",
    "print(\"Initial time split sizes -> train:\", X_tr.shape, \" test:\", X_te.shape)\n",
    "print(\"Initial class counts (train/test):\", y_tr.value_counts().to_dict(), y_te.value_counts().to_dict())\n",
    "\n",
    "# If test has fewer than 2 classes, fallback to stratified random split so evaluation is meaningful\n",
    "if y_te.nunique() < 2:\n",
    "    print(\"WARNING: time-based test has <2 classes. Falling back to stratified random split.\")\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_time, y_time, test_size=0.2, random_state=42, stratify=y_time)\n",
    "    print(\"Stratified split sizes -> train:\", X_tr.shape, \" test:\", X_te.shape)\n",
    "    print(\"Class counts (train/test):\", y_tr.value_counts().to_dict(), y_te.value_counts().to_dict())\n",
    "\n",
    "# --- 9) Force numeric types again and convert all to float64 to satisfy xgboost ---\n",
    "for c in X_tr.columns:\n",
    "    X_tr.loc[:, c] = pd.to_numeric(X_tr[c], errors='coerce').astype('float64')\n",
    "    X_te.loc[:, c] = pd.to_numeric(X_te[c], errors='coerce').astype('float64')\n",
    "\n",
    "# --- 10) Drop columns that are entirely NaN in training set (cannot impute meaningful value) ---\n",
    "all_nan = [c for c in X_tr.columns if X_tr[c].isna().all()]\n",
    "if all_nan:\n",
    "    print(\"Dropping all-NaN columns:\", all_nan)\n",
    "    X_tr.drop(columns=all_nan, inplace=True)\n",
    "    X_te.drop(columns=all_nan, inplace=True)\n",
    "\n",
    "# --- 11) Impute medians using training set ONLY ---\n",
    "numeric_cols = X_tr.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numeric_cols) == 0:\n",
    "    raise RuntimeError(\"No numeric columns available for training after coercion.\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_tr[numeric_cols])\n",
    "X_tr.loc[:, numeric_cols] = imputer.transform(X_tr[numeric_cols])\n",
    "X_te.loc[:, numeric_cols] = imputer.transform(X_te[numeric_cols])\n",
    "\n",
    "print(\"Any NaN left in X_tr after impute?\", X_tr.isna().any().any())\n",
    "print(\"Any NaN left in X_te after impute?\", X_te.isna().any().any())\n",
    "print(\"Final feature dtypes:\\n\", X_tr.dtypes)\n",
    "\n",
    "# --- 12) Final check: ensure y are ints and have at least 2 classes ---\n",
    "y_tr = y_tr.astype(int)\n",
    "y_te = y_te.astype(int)\n",
    "print(\"Final class distribution (train):\\n\", y_tr.value_counts())\n",
    "print(\"Final class distribution (test):\\n\", y_te.value_counts())\n",
    "\n",
    "# --- 13) Train RandomForest baseline ---\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_tr, y_tr)\n",
    "y_pred_rf = rf.predict(X_te)\n",
    "print(\"\\nRF accuracy:\", round(accuracy_score(y_te, y_pred_rf),4))\n",
    "print(classification_report(y_te, y_pred_rf))\n",
    "print(\"RF confusion matrix:\\n\", confusion_matrix(y_te, y_pred_rf))\n",
    "\n",
    "# --- 14) Train Stacking ensemble (ensure all columns numeric floats) ---\n",
    "for c in X_tr.columns:\n",
    "    X_tr.loc[:, c] = X_tr[c].astype('float64')\n",
    "    X_te.loc[:, c] = X_te[c].astype('float64')\n",
    "\n",
    "rf_b = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "xg = xgb.XGBClassifier(n_estimators=150, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "lg = lgb.LGBMClassifier(n_estimators=150, random_state=42)\n",
    "\n",
    "estimators = [('rf', rf_b), ('xg', xg), ('lg', lg)]\n",
    "stack = StackingClassifier(estimators=estimators,\n",
    "                           final_estimator=LogisticRegression(max_iter=1000),\n",
    "                           cv=5, n_jobs=-1, passthrough=False)\n",
    "\n",
    "print(\"\\nTraining stacking ensemble...\")\n",
    "stack.fit(X_tr, y_tr)\n",
    "print(\"Done training.\")\n",
    "\n",
    "# --- 15) Evaluate ---\n",
    "y_pred_stack = stack.predict(X_te)\n",
    "acc_stack = accuracy_score(y_te, y_pred_stack)\n",
    "print(\"\\nStacking ensemble accuracy:\", round(acc_stack, 4))\n",
    "print(classification_report(y_te, y_pred_stack))\n",
    "print(\"Ensemble confusion matrix:\\n\", confusion_matrix(y_te, y_pred_stack))\n",
    "\n",
    "# --- 16) Save model + imputer + feature order ---\n",
    "save_dict = {'model': stack, 'imputer': imputer, 'features': X_tr.columns.tolist()}\n",
    "joblib.dump(save_dict, model_path)\n",
    "print(\"\\nSaved ensemble + imputer to:\", model_path)\n",
    "\n",
    "# --- 17) Example single-row inference (coerce -> impute -> predict) ---\n",
    "sample = {'CO(GT)': 2.8, 'NO2(GT)': 55.0, 'PT08.S5(O3)': 650, 'T': 11.2, 'RH': 36.0, 'AH': 0.75}\n",
    "sample_df = pd.DataFrame([sample])\n",
    "sample_df = sample_df[[c for c in X_tr.columns if c in sample_df.columns]]\n",
    "for c in sample_df.columns:\n",
    "    sample_df.loc[:, c] = pd.to_numeric(sample_df[c], errors='coerce').astype('float64')\n",
    "sample_df.loc[:, numeric_cols] = imputer.transform(sample_df[numeric_cols])\n",
    "pred_next = stack.predict(sample_df)[0]\n",
    "probs_next = stack.predict_proba(sample_df)[0]\n",
    "label_map = {0:'Good', 1:'Moderate', 2:'Unhealthy'}\n",
    "print(\"\\nSample next-hour prediction:\")\n",
    "print(sample_df)\n",
    "print(\"Predicted next-hour class:\", pred_next, label_map[pred_next])\n",
    "print(\"Class probs:\", probs_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d25f6c0-51f5-4e75-aa2d-e96c5bf4ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48e6158-b0bc-4d11-be0d-ca7c7918dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "csv_path = 'C:/Users/91955/Desktop/infosys_aqi_project/data/AirQualityUCI.csv'\n",
    "model_path = r\"C:/Users/91955/Desktop/infosys_aqi_project/models/stack_ensemble_aqi.joblib\"\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3927280d-cc7b-4abd-b9e0-5b784237926a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>18.00.00</td>\n",
       "      <td>2,6</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>11,9</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>13,6</td>\n",
       "      <td>48,9</td>\n",
       "      <td>0,7578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>19.00.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>9,4</td>\n",
       "      <td>955.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>13,3</td>\n",
       "      <td>47,7</td>\n",
       "      <td>0,7255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>20.00.00</td>\n",
       "      <td>2,2</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>9,0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>11,9</td>\n",
       "      <td>54,0</td>\n",
       "      <td>0,7502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>21.00.00</td>\n",
       "      <td>2,2</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9,2</td>\n",
       "      <td>948.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>11,0</td>\n",
       "      <td>60,0</td>\n",
       "      <td>0,7867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/03/2004</td>\n",
       "      <td>22.00.00</td>\n",
       "      <td>1,6</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6,5</td>\n",
       "      <td>836.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>11,2</td>\n",
       "      <td>59,6</td>\n",
       "      <td>0,7888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time CO(GT)  PT08.S1(CO)  NMHC(GT) C6H6(GT)  PT08.S2(NMHC)  \\\n",
       "0  10/03/2004  18.00.00    2,6       1360.0     150.0     11,9         1046.0   \n",
       "1  10/03/2004  19.00.00      2       1292.0     112.0      9,4          955.0   \n",
       "2  10/03/2004  20.00.00    2,2       1402.0      88.0      9,0          939.0   \n",
       "3  10/03/2004  21.00.00    2,2       1376.0      80.0      9,2          948.0   \n",
       "4  10/03/2004  22.00.00    1,6       1272.0      51.0      6,5          836.0   \n",
       "\n",
       "   NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)     T    RH  \\\n",
       "0    166.0        1056.0    113.0        1692.0       1268.0  13,6  48,9   \n",
       "1    103.0        1174.0     92.0        1559.0        972.0  13,3  47,7   \n",
       "2    131.0        1140.0    114.0        1555.0       1074.0  11,9  54,0   \n",
       "3    172.0        1092.0    122.0        1584.0       1203.0  11,0  60,0   \n",
       "4    131.0        1205.0    116.0        1490.0       1110.0  11,2  59,6   \n",
       "\n",
       "       AH  Unnamed: 15  Unnamed: 16  \n",
       "0  0,7578          NaN          NaN  \n",
       "1  0,7255          NaN          NaN  \n",
       "2  0,7502          NaN          NaN  \n",
       "3  0,7867          NaN          NaN  \n",
       "4  0,7888          NaN          NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1) Load and canonicalize raw data ---\n",
    "df = pd.read_csv(csv_path, sep=';')\n",
    "df = df.replace(-200, np.nan)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f1747a-11f0-4d46-adba-3e3360334a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove trailing/leading spaces and stray semicolons in object columns\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == object:\n",
    "        df[c] = df[c].astype(str).str.strip().str.replace(';','', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5be8495-0a57-42f5-8d12-e6c5476b2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Convert numeric columns you plan to use to numeric (coerce invalid -> NaN) ---\n",
    "candidate_features = ['CO(GT)', 'NO2(GT)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n",
    "for c in candidate_features:\n",
    "    if c in df.columns:\n",
    "        # handle categorical dtype as well\n",
    "        if pd.api.types.is_categorical_dtype(df[c]):\n",
    "            df[c] = df[c].cat.as_ordered().codes\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5271ddc-7a1f-4c9d-8397-c083f43e258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Compute IAQI buckets (your deterministic rules) ---\n",
    "# safe guards: if column missing, create NaN/placeholder\n",
    "df['IAQI_CO'] = np.nan\n",
    "df['IAQI_NO2'] = np.nan\n",
    "df['IAQI_O3'] = np.nan\n",
    "\n",
    "if 'CO(GT)' in df.columns:\n",
    "    df.loc[:, 'IAQI_CO'] = np.where(df['CO(GT)'] <= 2, 25,\n",
    "                                    np.where(df['CO(GT)'] <= 4, 75, 125))\n",
    "\n",
    "if 'NO2(GT)' in df.columns:\n",
    "    df.loc[:, 'IAQI_NO2'] = np.where(df['NO2(GT)'] <= 40, 25,\n",
    "                                     np.where(df['NO2(GT)'] <= 80, 75, 125))\n",
    "\n",
    "if 'PT08.S5(O3)' in df.columns:\n",
    "    try:\n",
    "        # qcut on non-NA values only; use duplicates='drop' to prevent errors\n",
    "        df.loc[:, 'O3_bucket'] = pd.qcut(df['PT08.S5(O3)'], q=3, labels=[0,1,2], duplicates='drop')\n",
    "        df.loc[:, 'IAQI_O3'] = df['O3_bucket'].map({0:25, 1:75, 2:125})\n",
    "    except Exception:\n",
    "        df.loc[:, 'IAQI_O3'] = 75\n",
    "else:\n",
    "    df.loc[:, 'IAQI_O3'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d0c548-90f2-4560-88cb-55d66efda268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4) AQI and class mapping ---\n",
    "df.loc[:, 'AQI_calc'] = df[['IAQI_CO','IAQI_NO2','IAQI_O3']].max(axis=1)\n",
    "\n",
    "def aqi_to_label(v):\n",
    "    if pd.isna(v): return np.nan\n",
    "    if v <= 50: return 0\n",
    "    if v <= 100: return 1\n",
    "    return 2\n",
    "\n",
    "df.loc[:, 'AQI_class'] = df['AQI_calc'].apply(aqi_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcada3ec-6801-4417-b884-417455b4d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) Create next-hour target so target is not identical to features (prevents trivial mapping) ---\n",
    "df = df.reset_index(drop=True)\n",
    "df.loc[:, 'AQI_next'] = df['AQI_class'].shift(-1)\n",
    "df_time = df.dropna(subset=['AQI_next']).copy()\n",
    "df_time.loc[:, 'AQI_next'] = df_time['AQI_next'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e53c3362-fc46-40d2-8d8d-ec861fe12b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6) Build feature matrix & target ---\n",
    "features = [f for f in candidate_features if f in df_time.columns]\n",
    "if len(features) == 0:\n",
    "    raise RuntimeError(\"No feature columns found. Check CSV columns.\")\n",
    "\n",
    "X_time = df_time[features].copy()\n",
    "y_time = df_time['AQI_next'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e940517-5841-4ada-ab4c-da8dab5695dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 7) Ensure feature columns are numeric (force) BEFORE split to avoid strange categorical dtypes ---\n",
    "for c in X_time.columns:\n",
    "    # strip & coerce once more\n",
    "    X_time.loc[:, c] = pd.to_numeric(X_time[c], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98b3ec3a-49a7-45cc-ad1e-ab3dbccba366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train: (7576, 6)  test: (1894, 6)\n",
      "Initial class counts (train/test): {2.0: 7243, 1.0: 270, 0.0: 63} {2.0: 1894}\n",
      "WARNING: time-based test has <2 classes. Falling back to stratified random split.\n",
      "Stratified split sizes -> train: (7576, 6)  test: (1894, 6)\n",
      "Class counts (train/test): {2.0: 7310, 1.0: 216, 0.0: 50} {2.0: 1827, 1.0: 54, 0.0: 13}\n"
     ]
    }
   ],
   "source": [
    "# --- 8) Time-based split (chronological). If later test contains <2 classes, fallback to stratified split. ---\n",
    "split_idx = int(0.8 * len(X_time))\n",
    "X_tr, X_te = X_time.iloc[:split_idx].copy(), X_time.iloc[split_idx:].copy()\n",
    "y_tr, y_te = y_time.iloc[:split_idx].copy(), y_time.iloc[split_idx:].copy()\n",
    "print(\" train:\", X_tr.shape, \" test:\", X_te.shape)\n",
    "print(\"Initial class counts (train/test):\", y_tr.value_counts().to_dict(), y_te.value_counts().to_dict())\n",
    "\n",
    "# If test has fewer than 2 classes, fallback to stratified random split so evaluation is meaningful\n",
    "if y_te.nunique() < 2:\n",
    "    print(\"WARNING: time-based test has <2 classes. Falling back to stratified random split.\")\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_time, y_time, test_size=0.2, random_state=42, stratify=y_time)\n",
    "    print(\"Stratified split sizes -> train:\", X_tr.shape, \" test:\", X_te.shape)\n",
    "    print(\"Class counts (train/test):\", y_tr.value_counts().to_dict(), y_te.value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8020e309-24f8-438f-9bb3-698b76000bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9) Force numeric types again and convert all to float64 to satisfy xgboost ---\n",
    "for c in X_tr.columns:\n",
    "    X_tr.loc[:, c] = pd.to_numeric(X_tr[c], errors='coerce').astype('float64')\n",
    "    X_te.loc[:, c] = pd.to_numeric(X_te[c], errors='coerce').astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f408ce21-bd0c-4608-b7d5-923dd50f3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10) Drop columns that are entirely NaN in training set (cannot impute meaningful value) ---\n",
    "all_nan = [c for c in X_tr.columns if X_tr[c].isna().all()]\n",
    "if all_nan:\n",
    "    print(\"Dropping all-NaN columns:\", all_nan)\n",
    "    X_tr.drop(columns=all_nan, inplace=True)\n",
    "    X_te.drop(columns=all_nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d5d7410-ab9e-4295-b7a2-569b71e2f107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN left in X_tr after impute? False\n",
      "Any NaN left in X_te after impute? False\n",
      "Final feature dtypes:\n",
      " CO(GT)         float64\n",
      "NO2(GT)        float64\n",
      "PT08.S5(O3)    float64\n",
      "T              float64\n",
      "RH             float64\n",
      "AH             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 11) Impute medians using training set ONLY ---\n",
    "numeric_cols = X_tr.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numeric_cols) == 0:\n",
    "    raise RuntimeError(\"No numeric columns available for training after coercion.\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_tr[numeric_cols])\n",
    "X_tr.loc[:, numeric_cols] = imputer.transform(X_tr[numeric_cols])\n",
    "X_te.loc[:, numeric_cols] = imputer.transform(X_te[numeric_cols])\n",
    "\n",
    "print(\"Any NaN left in X_tr after impute?\", X_tr.isna().any().any())\n",
    "print(\"Any NaN left in X_te after impute?\", X_te.isna().any().any())\n",
    "print(\"Final feature dtypes:\\n\", X_tr.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbfc519e-07e0-4c51-808e-af4dc948a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final class distribution (train):\n",
      " AQI_next\n",
      "2    7310\n",
      "1     216\n",
      "0      50\n",
      "Name: count, dtype: int64\n",
      "Final class distribution (test):\n",
      " AQI_next\n",
      "2    1827\n",
      "1      54\n",
      "0      13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 12) Final check: ensure y are ints and have at least 2 classes ---\n",
    "y_tr = y_tr.astype(int)\n",
    "y_te = y_te.astype(int)\n",
    "print(\"Final class distribution (train):\\n\", y_tr.value_counts())\n",
    "print(\"Final class distribution (test):\\n\", y_te.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7ff7981-1aab-4ca4-9474-f96f21820bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RF accuracy: 0.953\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.08      0.08        13\n",
      "           1       0.11      0.04      0.06        54\n",
      "           2       0.97      0.99      0.98      1827\n",
      "\n",
      "    accuracy                           0.95      1894\n",
      "   macro avg       0.39      0.37      0.37      1894\n",
      "weighted avg       0.94      0.95      0.94      1894\n",
      "\n",
      "RF confusion matrix:\n",
      " [[   1    0   12]\n",
      " [   1    2   51]\n",
      " [   9   16 1802]]\n"
     ]
    }
   ],
   "source": [
    "# --- 13) Train RandomForest baseline ---\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_tr, y_tr)\n",
    "y_pred_rf = rf.predict(X_te)\n",
    "print(\"\\nRF accuracy:\", round(accuracy_score(y_te, y_pred_rf),4))\n",
    "print(classification_report(y_te, y_pred_rf))\n",
    "print(\"RF confusion matrix:\\n\", confusion_matrix(y_te, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f65aeca0-f468-4e3c-8192-49f601fc3780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training stacking ensemble...\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "# --- 14) Train Stacking ensemble (ensure all columns numeric floats) ---\n",
    "for c in X_tr.columns:\n",
    "    X_tr.loc[:, c] = X_tr[c].astype('float64')\n",
    "    X_te.loc[:, c] = X_te[c].astype('float64')\n",
    "\n",
    "rf_b = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "xg = xgb.XGBClassifier(n_estimators=150, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "lg = lgb.LGBMClassifier(n_estimators=150, random_state=42)\n",
    "\n",
    "estimators = [('rf', rf_b), ('xg', xg), ('lg', lg)]\n",
    "stack = StackingClassifier(estimators=estimators,\n",
    "                           final_estimator=LogisticRegression(max_iter=1000),\n",
    "                           cv=5, n_jobs=-1, passthrough=False)\n",
    "\n",
    "print(\"\\nTraining stacking ensemble...\")\n",
    "stack.fit(X_tr, y_tr)\n",
    "print(\"Done training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab4a31e5-3a9a-4bef-9156-4e6201b9f9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacking ensemble accuracy: 0.9641\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.00      0.00      0.00        54\n",
      "           2       0.97      1.00      0.98      1827\n",
      "\n",
      "    accuracy                           0.96      1894\n",
      "   macro avg       0.32      0.33      0.33      1894\n",
      "weighted avg       0.93      0.96      0.95      1894\n",
      "\n",
      "Ensemble confusion matrix:\n",
      " [[   0    0   13]\n",
      " [   1    0   53]\n",
      " [   1    0 1826]]\n"
     ]
    }
   ],
   "source": [
    "# --- 15) Evaluate ---\n",
    "y_pred_stack = stack.predict(X_te)\n",
    "acc_stack = accuracy_score(y_te, y_pred_stack)\n",
    "print(\"\\nStacking ensemble accuracy:\", round(acc_stack, 4))\n",
    "print(classification_report(y_te, y_pred_stack))\n",
    "print(\"Ensemble confusion matrix:\\n\", confusion_matrix(y_te, y_pred_stack))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a930dc7b-9cb8-4ccc-865d-5171098bfaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved ensemble + imputer to: C:/Users/91955/Desktop/infosys_aqi_project/models/stack_ensemble_aqi.joblib\n"
     ]
    }
   ],
   "source": [
    "# --- 16) Save model + imputer + feature order ---\n",
    "save_dict = {'model': stack, 'imputer': imputer, 'features': X_tr.columns.tolist()}\n",
    "joblib.dump(save_dict, model_path)\n",
    "print(\"\\nSaved ensemble + imputer to:\", model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
